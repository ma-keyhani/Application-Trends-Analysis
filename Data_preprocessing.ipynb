{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d927259",
   "metadata": {},
   "source": [
    "## Merging Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557bead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 Description:\n",
      "       date_applied    paid_apps     all_apps\n",
      "count           157   157.000000   157.000000\n",
      "unique          157          NaN          NaN\n",
      "top       3/08/2021          NaN          NaN\n",
      "freq              1          NaN          NaN\n",
      "mean            NaN   391.547771   406.738854\n",
      "std             NaN   487.809671   503.904704\n",
      "min             NaN     1.000000     1.000000\n",
      "25%             NaN    47.000000    52.000000\n",
      "50%             NaN    90.000000    98.000000\n",
      "75%             NaN   725.000000   747.000000\n",
      "max             NaN  2350.000000  2398.000000\n",
      "\n",
      "Dataset 2 Description:\n",
      "       date_applied    paid_apps     all_apps\n",
      "count           154   154.000000   154.000000\n",
      "unique          154          NaN          NaN\n",
      "top       2/08/2022          NaN          NaN\n",
      "freq              1          NaN          NaN\n",
      "mean            NaN   400.318182   418.064935\n",
      "std             NaN   482.405312   498.983800\n",
      "min             NaN     1.000000     1.000000\n",
      "25%             NaN    59.000000    63.250000\n",
      "50%             NaN   108.000000   123.500000\n",
      "75%             NaN   698.250000   719.500000\n",
      "max             NaN  2056.000000  2135.000000\n",
      "\n",
      "Merged dataset has been saved to merged_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to extract the financial year from the file name\n",
    "def extract_financial_year(file_name):\n",
    "    match = re.search(r'(\\d{4})', file_name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(f\"No financial year found in file name: {file_name}\")\n",
    "\n",
    "# File paths (replace with your actual file paths)\n",
    "file_path_1 = 'apps_by_day_2021.csv'\n",
    "file_path_2 = 'apps_by_day_2022.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Rename the columns to a common set of names\n",
    "df1.rename(columns={'date_applied_2021': 'date_applied', 'paid_apps_2021': 'paid_apps', 'all_apps_2021': 'all_apps'}, inplace=True)\n",
    "df2.rename(columns={'date_applied_2022': 'date_applied', 'paid_apps_2022': 'paid_apps', 'all_apps_2022': 'all_apps'}, inplace=True)\n",
    "\n",
    "# Describing each datasets before merging\n",
    "description1 = df1.describe(include='all')\n",
    "print(\"Dataset 1 Description:\")\n",
    "print(description1)\n",
    "description2 = df2.describe(include='all')\n",
    "print(\"\\nDataset 2 Description:\")\n",
    "print(description2)\n",
    "\n",
    "# Concatenate the datasets\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_file_path = 'merged_dataset.csv'\n",
    "merged_df.to_csv(merged_file_path, index=False)\n",
    "\n",
    "print(f\"\\nMerged dataset has been saved to {merged_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28889b4",
   "metadata": {},
   "source": [
    "## Finding duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04abaac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records found: 0\n",
      "Duplicate records saved to duplicate_records.csv\n"
     ]
    }
   ],
   "source": [
    "df = merged_df\n",
    "# Find duplicate records\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "# Remove duplicates from the original DataFrame\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save duplicate records to a CSV file\n",
    "duplicates.to_csv('duplicate_records.csv', index=False)\n",
    "\n",
    "# Show the number of duplicate records\n",
    "num_duplicates = len(duplicates)\n",
    "print(f\"Number of duplicate records found: {num_duplicates}\")\n",
    "\n",
    "print(\"Duplicate records saved to duplicate_records.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242e43f",
   "metadata": {},
   "source": [
    "## Describing new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d0d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description:\n",
      "       date_applied    paid_apps     all_apps\n",
      "count           311   311.000000   311.000000\n",
      "unique          311          NaN          NaN\n",
      "top       3/08/2021          NaN          NaN\n",
      "freq              1          NaN          NaN\n",
      "mean            NaN   395.890675   412.347267\n",
      "std             NaN   484.378046   500.696819\n",
      "min             NaN     1.000000     1.000000\n",
      "25%             NaN    51.500000    57.000000\n",
      "50%             NaN    98.000000   108.000000\n",
      "75%             NaN   721.500000   737.500000\n",
      "max             NaN  2350.000000  2398.000000\n",
      "\n",
      "Columns' Data Types:\n",
      "date_applied    object\n",
      "paid_apps        int64\n",
      "all_apps         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = merged_df\n",
    "# Describe the dataset\n",
    "description = df.describe(include='all')\n",
    "print(\"Dataset Description:\")\n",
    "print(description)\n",
    "\n",
    "# Print the columns' data types\n",
    "print(\"\\nColumns' Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814d4bc",
   "metadata": {},
   "source": [
    "## Assessing date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6dc2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First date in the dataset: 2021-08-03 00:00:00\n",
      "Last date in the dataset: 2023-01-17 00:00:00\n",
      "\n",
      "No false dates found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "date_column_name = 'date_applied'\n",
    "\n",
    "# Convert the date column to datetime format for easier manipulation\n",
    "df[date_column_name] = pd.to_datetime(df[date_column_name], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Find and print the first and last date of the dataset\n",
    "first_date = df[date_column_name].min()\n",
    "last_date = df[date_column_name].max()\n",
    "print(f\"\\nFirst date in the dataset: {first_date}\")\n",
    "print(f\"Last date in the dataset: {last_date}\")\n",
    "\n",
    "# Check and print any false dates in the dataset\n",
    "false_dates = df[df[date_column_name].isna()]\n",
    "if not false_dates.empty:\n",
    "    print(\"\\nFalse dates found in the dataset:\")\n",
    "    print(false_dates)\n",
    "else:\n",
    "    print(\"\\nNo false dates found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32485cd",
   "metadata": {},
   "source": [
    "## Assessing Apps columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f12e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Records where paid_apps_2021 is greater than all_apps_2021:\n",
      "    date_applied  paid_apps  all_apps\n",
      "124   2021-12-09          4         1\n",
      "126   2021-12-13          2         1\n",
      "130   2021-12-17          2         1\n",
      "133   2021-12-20          2         1\n",
      "134   2021-12-21          5         2\n",
      "138   2021-12-27          2         1\n",
      "142   2022-01-03          3         1\n",
      "143   2022-01-05        393         1\n",
      "144   2022-01-06        135         3\n",
      "145   2022-01-07        111         2\n",
      "149   2022-01-11         69        67\n",
      "150   2022-01-12         60        57\n",
      "155   2022-01-17        103        87\n",
      "156   2022-01-18         92        44\n",
      "282   2022-12-08          2         1\n",
      "286   2022-12-14          2         1\n",
      "287   2022-12-15          2         1\n",
      "288   2022-12-16          6         3\n",
      "292   2022-12-22          3         1\n",
      "297   2023-01-04        433         1\n",
      "298   2023-01-05        163         1\n",
      "299   2023-01-06        124         1\n",
      "303   2023-01-10         98        74\n",
      "304   2023-01-11         87        64\n",
      "309   2023-01-16        122        72\n",
      "310   2023-01-17        104        45\n",
      "\n",
      "Invalid records have been saved to false_records.csv and removed from the dataset.\n",
      "\n",
      "Cleaned dataset with cumulative columns has been saved to cleaned_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "# Find and print records where paid_apps_2021 > all_apps_2021\n",
    "invalid_records = df[df['paid_apps'] > df['all_apps']]\n",
    "if not invalid_records.empty:\n",
    "    print(\"\\nRecords where paid_apps_2021 is greater than all_apps_2021:\")\n",
    "    print(invalid_records)\n",
    "    # Save these invalid records to a CSV file and remove them from the dataset\n",
    "    invalid_records.to_csv('false_records.csv', index=False)\n",
    "    df = df.drop(invalid_records.index)\n",
    "    print(\"\\nInvalid records have been saved to false_records.csv and removed from the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNo records found where paid_apps_2021 is greater than all_apps_2021.\")\n",
    "\n",
    "    \n",
    "# Save the cleaned and updated dataset to a new CSV file\n",
    "df.to_csv('cleaned_dataset.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned dataset with cumulative columns has been saved to cleaned_dataset.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
